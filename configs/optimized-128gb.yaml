# GPT-Mini Optimized Configuration for 128GB VRAM
# Optimized for DGX with 128GB VRAM - uses ~108GB for maximum throughput
# Expected: ~45% faster training than default config

# Model Architecture
model:
  name: "gpt_mini_2"
  vocab_size: 100277
  n_layer: 12
  n_head: 12
  n_embd: 768
  block_size: 2048
  dropout: 0.1

# Training Hyperparameters - OPTIMIZED FOR SPEED
training:
  # Optimized batch settings for 128GB VRAM
  batch_size: 12
  gradient_accumulation_steps: 4
  # Effective batch = 12 × 4 = 48

  max_iters: 100000

  # Optimizer - same as default
  learning_rate: 0.0003
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0

  # Learning Rate Schedule
  warmup_iters: 2000
  lr_decay_iters: 100000
  min_lr: 0.00003

# Evaluation
evaluation:
  eval_interval: 500
  eval_iters: 100

  # Text Generation During Eval
  generate_samples: true
  num_generation_prompts: 3
  generation_max_tokens: 100
  generation_temperature: 0.8
  generation_top_k: 40
  generation_top_p: 0.9

# Logging
logging:
  log_interval: 10
  backends:
    - console
    - wandb

  wandb_project: "gpt-mini-optimized"
  wandb_entity: null
  wandb_mode: "online"

  tensorboard_dir: "./runs"
  csv_log_dir: "./logs"

# Checkpointing
checkpointing:
  checkpoint_dir: "./checkpoints/train_val_split"
  checkpoint_interval: 1000
  keep_last_n_checkpoints: 5

# Data
data:
  data_dir: "./datasets/fineweb"  # Fallback if train/val dirs not specified
  train_data_dir: "./datasets/fineweb/train" # Training data directory (if null, uses data_dir)
  val_data_dir: "./datasets/fineweb/val" # Validation data directory (if null, uses data_dir)
  num_workers: 4

# System
system:
  device: "cuda"
  compile: true

# Performance Notes:
# - Effective batch size: 32 (8 × 4 gradient accumulation steps)
# - block_size: 2048 (standard power of 2)
